---
layout: default
---

## Introduction

On this page, I’ll give a short overview about the course _Command Line Tools for Linguists_, which I took in august term, 2019. The course aims to give a gentle and practical introduction to the UNIX command line environment for absolute beginners, like myself. In my studies, I have run into many kinds of programming and I have always felt like I understood nothing, even though I maybe should have. So, taking this course, I wanted to be able to have a basic literacy of UNIX commands, to complete simple, basic tasks myself, and to understand simple instructions when things go wrong. 

I also thought that this course might give me some confidence in understanding and using Python and R, which I have found useful in some projects.

Below I’ll summarize what I have learned and how I felt about it.

## Week 1: Introduction to Command Line Environments

The first week of the course concentrated, first of all, on getting to know what a command line is and how to set it up on the computer. In my case, the set-up was easy, since I’m working on MacOS – I simply needed to open up the Terminal and I was ready to go.<br>
Secondly, I got to know some commands used for basic navigation, like **whoami, pwd, cd, mkdir, ls, rm**, and **mv**. In my opinion, these commands are a corner stone of working with the command line. In the autumn, I was also doing topic modelling with MALLET for another course, and knowing these commands helped me out a lot. Since MALLET is operated straight through the command line, making new directories and navigating between multiple directories was easy. I felt much more confident in working with MALLET since I knew how to navigate my environment.

For example, I needed to create a new directory for my own data which I used in the topic modelling. The process was a bit like this:

    $ pwd      	      	
    ~/mallet-2.0.8
    $ mkdir mydata
    $ cd mydata
    $ pwd		
    ~/mallet-2.0.8/mydata

By these commands, I could first check my current working directory, then create a new directory, change to that directory, and check again that I'm where I want to be. The command **pwd** was really handy, since all MALLET commands need to be run from the mallet-directory and not from any of its sub-directories!

I also learned how to inspect the contents of text files using **less**, how to fetch contents from the web using **wget**, and how to use the text editor **emacs**, which I'm actually using to write the contents for this page.

## Week 2: Navigating a UNIX System

This week had three main topics: more information on the UNIX file system, basic information about processes, and  working with remote servers. I learned how to copy and remove directories (since that is a bit different from copying and removing files!) using the **-R** option after commands, I took a tour around my /tmp, /usr, and /bin directories, and I learned a bit about processes (what are they actually?).

I had already worked with a remote server one year ago, but now I was finally able to understand what it actually means. A year ago, I was working on the CSC remote server Taito, but it seemed a bit like black magic, since I had no idea what was really going on. Working with remote serves can be really helpful especially since laptop computers’ computing powers are no match to CSC’s super-computers efficacy. Connecting to remote servers through the command line is also very easy. For example, if I wanted to connect to basically unlimited computing superpower, I can simply type this to my terminal:

    $ ssh <myCSCusername>@puhti.csc.fi

And that's it! Since my username for CSC is different from my username on my computer, I have to specify it. If they were the same, I could simply type:

    $ ssh puhti.csc.fi

which would connect me to the Puhti server. 

## Week 3: Basic Corpus Processing

The third week of the course introduced some basic commands for processing plain text files and structured text files, i.e. tables. I learned how to create word lists from text files using commands like **sort** and **uniq**, and how to replace characters easily with **tr** (making removing punctuation or converting everything to lowercase a lot easier than working with Word). 

For example, this command would convert all uppercase letters to lowercase:

    $ tr “[:upper:]” “[:lower:]” 

I also learned about regular expressions, which can be used, for example, to match certain patterns sequences of characters in a search. For example, regular expressions can be used to find words in a certain case when combined with the command **egrep**. Since Finnish cases are subject to vowel harmony, there are usually two possible endings for a case, and regular expressions can be flexibly used to find both variations of a case ending. For instance, the ablative case can be expressed with either _-lta_ or _-ltä_. The following command would search for both of these patterns:

    $ egrep “lt[aä]$”

The $ sign at the end of the expression signals word boundary, so the command would only find words ending in this pattern. 

I also learned how to pipe commands, like this:

    $ egrep “lt[aä]$” text1.txt | wc -l

Here, the command **egrep** first searches for a pattern in a text file, and then uses the output of that command as an input for the **wc -l** command, which then outputs how many matches were found in the text file.

In my opinion, this week's material was really useful for a linguist. However, I still need a lot of practice with regular expressions.

## Week 4: Advanced Corpus Processing

Like the title implies, the fourth week built upon the topics of the previous week. This included combining more commands with piping which makes more complex operations are lot more streamlined by eliminating intermediate steps, and more regular expressions, thus more struggles for me. The command we focused on during the fourth week was **sed**, which can be used with a lot of options for multiple purposes. **sed** can be used with regular expressions, like **egrep**, but also for many other things. Some examples are listed in the table below.

| Options | Function | Example syntax | What does it  do? |
| ------- | ------ | ------ | ------ |
| `s` | Substitute | `sed 's/x/y'` | Replace x with y |
| `d` | Delete | `sed '/x/d'` | Delete x |
| `g` | Global | `sed 's/x/y/g'` | Replace every occurrence of x with y |
| `-E` | Extended Regex | `sed -E 's/[a-z]+d/p` | Enable using extended regular expressions |
| `-n` | No printing | `sed -n '/x/'` | Suppress printing |

Because **sed** supports so many different functions and extended regular expressions, the commands can get a bit messy pretty soon. I personally had many problems with composing sed commands, trying to remember all possible options and flags, while testing out multiple variations of a regular expression to correctly match a pattern – and just when I thought I had done everything right, I got an error because I had forgot to type a second `'`. 

Here's an example of how intimidating **sed** commands can look like at first:

    $ sed -nE 's/that ([a-z]+d)([ \.,?!:$])/that \1 not\2/gp' test.txt > modified_text.txt

This command does the following:
* It finds the pattern  _'that ...d'_, e.g. _'that happened'_...
* ... that is followed by a space, a punctuation mark, or is at the end of a line...
* ... in a file called test.txt.
* It substitutes those patterns with _'not ...d', e.g. _'that **not** happened'_
* .. and saves the modified result to a text file called modified_test.txt.
* The command uses extended regular expressions,
* It doesn't print the output and flood the Terminal window masses of text,
* And it operates globally, i.e. it applies to every occurrence of _'that ...d'_ in a line, not just the first one.

**sed** alone would have been enough for me to handle, but the fourth week also introduced more complex pipelines of corpus processing, like creating frequency lists and n-grams. Luckily, these processes mostly included commands that were introduced in the previous weeks. However, I felt like I needed more examples in order to apply the pipelines myself, without just copying the commands from instructions. 

## Week 5: Scripting and Configuration Files

The fifth week of the course moved into the realm of what I consider actual programming – we were introduced to writing scripts, configuration files, and environment variables. For me, this week was really demanding, and I felt like I needed a lot more practice and examples in order to be able to write scripts myself. One of the problems was of course caused by still struggling with getting **sed** commands and regular expressions right, since these were needed in most of the exercises. However, in the end, after hours and hours of trial and error I was able to create some simple scripts myself, even though they were not as neatly formatted as they could have been, or that could have solved more complex problems. 

I was able to create one simple script myself:

    #!/bin/bash
    
    sort $1 |
    uniq |
    wc -l

This script takes the first column of a table as input and as output gives how many different data points there are. I created this for my work, since one of the researchers I’m assisting wanted to know how many authors she has listed in a huge Excel-file which contains all her data. The authors are listed in the first column of the file. The number returned by this script tells how many individual authors are featured in the file.

I also learned how to customize my bash prompt (with which I was much more successful than with scripts) and how to change the values of my existing environment variables. 

Overall, I felt kind of discouraged by my own lack of progress, especially since scripts can be so useful in automating processes that would otherwise need typing a lot of commands one by one.

## Week 6: Installing and Running Programs

This week included some topics that I was already kind of familiar with, as well as topics that were completely new for me. For example, installing Python packages with **pip** and using the Python interpreter was something I had done before, since Python was the first programming language I ever encountered. There was a comforting familiarity with typing:

    $ pip install nltk
    $ python3
    >>> import nltk

once again. These commands install NLTK ([Natural Language Toolkit](https://www.nltk.org){:target="_blank"}, good stuff) and start the Python interpreter (the change can be seen in the bash prompt, which turns to `>>>`). Then `import`, like the name says, imports NLTK. All Python modules need to be imported first in order to be used. 

In addition these commands, I had also already used **brew** to install software. However, the concept of root user and commands related to that (e.g. **su**, **sudo**) were new to me, as was the concept of a Makefile and the process of creating one myself.

Makefiles turned out to be the problem for me during this week (in addition to some Mac-specific problems with some software…). I think this was mostly due to me struggling with scripting the previous week, so I felt like I didn’t quite grasp the fundamentals. I think I still need a lot more practice in this area.

## Week 7: Version Control

The final week of the course concentrated on version control, in short, managing changes when developing eg. code, web sites, or other projects. I got to learn about Git, how to use it through my command line, and how it interacts with GitHub. GitHub was of course a website I had run into before, since every time I had googled something related to programming, I almost always encountered a link leading to GitHub. However, I had no idea I could upload files to GitHub using command line. I also didn’t know about branches. Branches can be useful when one wants to try out new things or develop something a bit experimental but doesn’t want that to mess up with the progress already made. Using branches is also quite simple, once I got the hang of it. 

For example, if I had a project to which I would like to add new features without compromising the stuff I already have up and running, I could create a new branch for my new feature:

    $ git branch new-feature
    $ git checkout new-feature

The command **git branch** creates a new branch with a desired name, and the command **git checkout** switches to the new branch. Now I could try out new things without worrying that I will mess up my previous work. When done with developing the new features, I could merge the branches:

    $ git checkout master
    $ git merge new-feature

The first command switches back to the master branch, and the second command merges the two branches, so that my new changes are also in the master branch.

In my opinion, working with Git and GitHub was fun, and quite soon it already turned out to be very useful. I actually had to create a GitHub repository for the topic modelling project I mentioned earlier. At the moment, I have three GitHub repositories.

![Repos](https://github.com/raaoo/raaoo.github.io/cmdline-course/assets/img/repos.jpg )



## Final Assignment: Building Webpages using GitHub Pages